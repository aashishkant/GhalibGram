{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP+faNHnYa0GDRNzorjhpu0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aashishkant/GhalibGram/blob/master/mirza2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZjLCW3ej_otJ",
        "outputId": "a412193e-a7fa-4a34-82f2-e37f5e3313d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.35.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.1)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.11.17)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N-7-M_uY_Ryy"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.nn import GRU, Embedding\n",
        "from transformers import TrainingArguments\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the output directory to the current working directory\n",
        "fine_tuned_model_output_dir = \"./\""
      ],
      "metadata": {
        "id": "hf5LSlrMC-BL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Read data from the Oscar text file\n",
        "with open(\"mirza.txt\", \"r\", encoding=\"utf-8\") as file:\n",
        "    data = file.read()"
      ],
      "metadata": {
        "id": "f97HOYZ-DH4b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_char_mapping(data):\n",
        "    unique_chars = list(set(data))\n",
        "    char_to_index = {char: idx for idx, char in enumerate(unique_chars)}\n",
        "    index_to_char = {idx: char for char, idx in char_to_index.items()}\n",
        "    return char_to_index, index_to_char\n",
        "#char_to_index, index_to_char = get_char_mapping(data)\n",
        "#print(get_char_mapping)print(char_to_index)"
      ],
      "metadata": {
        "id": "d3jilohUDMz1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "char_to_index, index_to_char = get_char_mapping(data)"
      ],
      "metadata": {
        "id": "J2RnlqZh_njk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "block_size = 64\n",
        "batch_size = 16\n",
        "max_iters = 5000\n",
        "learning_rate = 1e-4\n",
        "eval_iters = 250\n",
        "temperature = 1.0\n",
        "device = torch.device(\"cpu\")"
      ],
      "metadata": {
        "id": "zsRHAQWtAlUg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming 'data' is defined elsewhere in your code\n",
        "n = int(0.8 * len(data))\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n"
      ],
      "metadata": {
        "id": "8gynwRPcAohX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_batch(split):\n",
        "    data_split = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data_split) - block_size, (batch_size,))\n",
        "    x = torch.stack([torch.tensor([char_to_index[char] for char in data_split[i:i + block_size]]) for i in ix])\n",
        "    y = torch.stack([torch.tensor([char_to_index[char] for char in data_split[i + 1:i + block_size + 1]]) for i in ix])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y\n"
      ],
      "metadata": {
        "id": "iThFQ_6GAuBw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BigramLanguageModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_size, hidden_size):\n",
        "        super(BigramLanguageModel, self).__init__()\n",
        "        self.embedding = Embedding(vocab_size, embedding_size)\n",
        "        self.gru = GRU(embedding_size, hidden_size, batch_first=True)\n",
        "        self.dropout = nn.Dropout(p=0.5)\n",
        "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "    def forward(self, input, targets=None):\n",
        "        embedded = self.embedding(input)\n",
        "        gru_out, _ = self.gru(embedded)\n",
        "        gru_out = self.dropout(gru_out)\n",
        "        logits = self.fc(gru_out)\n",
        "\n",
        "        if targets is not None:\n",
        "            loss = F.cross_entropy(logits.view(-1, vocab_size), targets.view(-1))\n",
        "        else:\n",
        "            loss = None\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, input, max_new_tokens):\n",
        "        for _ in range(max_new_tokens):\n",
        "            logits, _ = self.forward(input)\n",
        "            logits = logits[:, -1, :]\n",
        "            probs = F.softmax(logits / temperature, dim=-1)\n",
        "            index_next = torch.multinomial(probs, num_samples=1)\n",
        "\n",
        "            # Fixing the dimension mismatch\n",
        "            index_next = index_next.view(-1, 1)\n",
        "\n",
        "            input = torch.cat((input, index_next), dim=1)\n",
        "        return input\n"
      ],
      "metadata": {
        "id": "JgfwZk06A0mv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define your vocabulary size based on the content of the Oscar text file\n",
        "vocab_size = len(set(data))\n",
        "embedding_size = 128\n",
        "hidden_size = 256\n",
        "\n",
        "model = BigramLanguageModel(vocab_size, embedding_size, hidden_size)\n",
        "model.to(device)\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)"
      ],
      "metadata": {
        "id": "RhrlPhywBABx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n"
      ],
      "metadata": {
        "id": "rZmdjbX2BKXf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "for iter in range(max_iters):\n",
        "    if iter % eval_iters == 0:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step: {iter}, train loss: {losses['train']:.3f}, val loss: {losses['val']:.3f}\")\n",
        "\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    logits, loss = model.forward(xb, yb)\n",
        "\n",
        "    # Add gradient clipping\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fd9lb20DBR0N",
        "outputId": "8d6e1e1d-baee-4e91-da1c-3da8d7fd2337"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step: 0, train loss: 0.087, val loss: 4.586\n",
            "step: 250, train loss: 0.086, val loss: 4.559\n",
            "step: 500, train loss: 0.085, val loss: 4.592\n",
            "step: 750, train loss: 0.086, val loss: 4.672\n",
            "step: 1000, train loss: 0.085, val loss: 4.622\n",
            "step: 1250, train loss: 0.085, val loss: 4.639\n",
            "step: 1500, train loss: 0.084, val loss: 4.666\n",
            "step: 1750, train loss: 0.084, val loss: 4.657\n",
            "step: 2000, train loss: 0.084, val loss: 4.657\n",
            "step: 2250, train loss: 0.083, val loss: 4.659\n",
            "step: 2500, train loss: 0.084, val loss: 4.665\n",
            "step: 2750, train loss: 0.083, val loss: 4.671\n",
            "step: 3000, train loss: 0.083, val loss: 4.688\n",
            "step: 3250, train loss: 0.083, val loss: 4.694\n",
            "step: 3500, train loss: 0.083, val loss: 4.616\n",
            "step: 3750, train loss: 0.083, val loss: 4.711\n",
            "step: 4000, train loss: 0.082, val loss: 4.680\n",
            "step: 4250, train loss: 0.083, val loss: 4.771\n",
            "step: 4500, train loss: 0.082, val loss: 4.711\n",
            "step: 4750, train loss: 0.082, val loss: 4.705\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(loss.item())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KgL4C026BiKy",
        "outputId": "2279bd4e-6794-4ec3-a134-cd99ddf6ec3e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.10828258842229843\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "generated_indices = model.generate(context, max_new_tokens=500)[0].tolist()\n",
        "generated_chars = ''.join([index_to_char[idx] for idx in generated_indices])\n",
        "print(generated_chars)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gMhlWYPg0cVU",
        "outputId": "642e958d-6576-4327-ae7f-53700914ed0f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hī pā se nikāle gar na ḳhaar ātish\n",
            "\n",
            "Sharar hai rañg ba.ad iz.hār-e-tāb-e-jalva-e-tamkīñ\n",
            "kare hai sañ teñ zulā kar āvāz gar nahīñ aatī\n",
            "\n",
            "Dāġh-e-dil gar nazar nahīñ aatā\n",
            "bū bhī ai chārā-gar nahīñ aatī\n",
            "\n",
            "Ham vahāñ haiñ jahāñ se ham ko bi\n",
            "āḳhir is dard kī davā kyā hai\n",
            "\n",
            "Ham haiñ mushtāq aur vo be-zār\n",
            "yā ilāhī ye mājrā kyā hai\n",
            "\n",
            "Maiñ bhī muñh meñ zabān rakhtā huuñ\n",
            "kaash pūchho ki mudda.ā kyā hai\n",
            "\n",
            "Jab ki tujh bin nahīñ koī maujūd\n",
            "phir ye hañgāma ai ḳhudā kyā hai\n",
            "\n",
            "Ye parī-chehra log kaise haiñ\n",
            "ġhamza o ishva o adā kyā hai\n",
            "\n",
            "Shikan-e-zulf-e-ambarīñ kyuuñ hai\n",
            "nigah-e-chashm-e-surma se kous kahāñ se aa.e haiñ\n",
            "abr kyā chiiz hai havā kyā hai\n",
            "āġhil gar nahīñ aatī\n",
            "\n",
            "Dāġh-e-dil gar nazar nahīñ aatā\n",
            "bū bir darvehrd yāġhāñ haiñ jahāñ se ham ko bhī\n",
            "kuchh hamārī ḳhabar nahīñ aatī\n",
            "\n",
            "Marte haiñ aarzū meñ marne hī baat jo chup huuñ\n",
            "varna kyā baat kar nahīñ aatī\n",
            "\n",
            "Kyuun na chīḳhūñ ki yaad karte haiñ\n",
            "merī āvāz gar nahīñ aatī\n",
            "\n",
            "Dāġh-e-dil gar nazar nahīñ aatā\n",
            "bū bhī ai chārā-gar nahīñ aatī\n",
            "\n",
            "Ham vahāñ haiñ jahāñ se ham k\n"
          ]
        }
      ]
    }
  ]
}